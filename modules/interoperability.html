<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Configuration" href="../configuration.html" /><link rel="prev" title="Generics" href="generics.html" />

    <!-- Generated with Sphinx 8.0.2 and Furo 2024.08.06 -->
        <title>Interoperability - Warp 1.4.1</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a91381f3" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-title-font-size: 100%;
  --admonition-font-size: 100%;
  --color-api-pre-name: #4e9a06;
  --color-api-name: #4e9a06;
  --color-admonition-title--seealso: #ffffff;
  --color-admonition-title-background--seealso: #448aff;
  --color-admonition-title-background--note: #76b900;
  --color-admonition-title--note: #ffffff;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #76b900;
  --color-admonition-title--note: #ffffff;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #76b900;
  --color-admonition-title--note: #ffffff;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Warp 1.4.1</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/logo-light-mode.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/logo-dark-mode.png" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Warp 1.4.1</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">User's Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="devices.html">Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="differentiability.html">Differentiability</a></li>
<li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribution_guide.html">Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Topics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../codegen.html">Code Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="allocators.html">Allocators</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../profiling.html">Profiling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Runtime Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions.html">Kernel Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Simulation Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="sim.html">warp.sim</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">warp.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="fem.html">warp.fem</a></li>
<li class="toctree-l1"><a class="reference internal" href="render.html">warp.render</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/warp">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/warp-lang">PyPI</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discord.com/channels/827959428476174346/1285719658325999686">Discord</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/modules/interoperability.rst.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/NVIDIA/warp/edit/main/docs/modules/interoperability.rst" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="interoperability">
<h1>Interoperability<a class="headerlink" href="#interoperability" title="Link to this heading">¶</a></h1>
<p>Warp can interoperate with other Python-based frameworks such as NumPy through standard interface protocols.</p>
<p>Warp supports passing external arrays to kernels directly, as long as they implement the <code class="docutils literal notranslate"><span class="pre">__array__</span></code>, <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code>, or <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocols.  This works with many common frameworks like NumPy, CuPy, or PyTorch.</p>
<p>For example, we can use NumPy arrays directly when launching Warp kernels on the CPU:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">saxpy</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">y</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Likewise, we can use CuPy arrays on a CUDA device:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cupy</span> <span class="k">as</span> <span class="nn">cp</span>

<span class="k">with</span> <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that with CUDA arrays, it’s important to ensure that the device on which the arrays reside is the same as the device on which the kernel is launched.</p>
<p>PyTorch supports both CPU and GPU tensors and both kinds can be passed to Warp kernels on the appropriate device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda:0&quot;</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<section id="numpy">
<h2>NumPy<a class="headerlink" href="#numpy" title="Link to this heading">¶</a></h2>
<p>Warp arrays may be converted to a NumPy array through the <code class="docutils literal notranslate"><span class="pre">warp.array.numpy()</span></code> method. When the Warp array lives on
the <code class="docutils literal notranslate"><span class="pre">cpu</span></code> device this will return a zero-copy view onto the underlying Warp allocation. If the array lives on a
<code class="docutils literal notranslate"><span class="pre">cuda</span></code> device then it will first be copied back to a temporary buffer and copied to NumPy.</p>
<p>Warp CPU arrays also implement  the <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code> protocol and so can be used to construct NumPy arrays
directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">2.</span> <span class="mf">3.</span><span class="p">]</span>
</pre></div>
</div>
<p>Data type conversion utilities are also available for convenience:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">warp_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span>
<span class="o">...</span>
<span class="n">numpy_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">dtype_to_numpy</span><span class="p">(</span><span class="n">warp_type</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">warp_type</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numpy_type</span><span class="p">)</span>
</pre></div>
</div>
<p>To create Warp arrays from NumPy arrays, use <a class="reference internal" href="#warp.from_numpy" title="warp.from_numpy"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_numpy()</span></code></a>
or pass the NumPy array as the <code class="docutils literal notranslate"><span class="pre">data</span></code> argument of the <a class="reference internal" href="runtime.html#warp.array" title="warp.array"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.array</span></code></a> constructor directly.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">arr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/context.py#L4628-L4667"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_numpy" title="Link to this definition">¶</a></dt>
<dd><p>Returns a Warp array created from a NumPy array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>arr</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v2.1)"><em>ndarray</em></a>) – The NumPy array providing the data to construct the Warp array.</p></li>
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.13)"><em>type</em></a><em> | </em><em>None</em>) – The data type of the new Warp array. If this is not provided, the data type will be inferred.</p></li>
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Sequence" title="(in Python v3.13)"><em>Sequence</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>] </em><em>| </em><em>None</em>) – The shape of the Warp array.</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – The device on which the Warp array will be constructed.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>) – Whether gradients will be tracked for this array.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The data type of the NumPy array is not supported.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numpy_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/types.py#L1080-L1087"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_numpy" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp dtype corresponding to a NumPy dtype.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/types.py#L1089-L1096"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_numpy" title="Link to this definition">¶</a></dt>
<dd><p>Return the NumPy dtype corresponding to a Warp dtype.</p>
</dd></dl>

</section>
<section id="pytorch">
<span id="pytorch-interop"></span><h2>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading">¶</a></h2>
<p>Warp provides helper functions to convert arrays to/from PyTorch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Torch tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Torch tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from PyTorch tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from PyTorch autograd tensors, allowing the use of Warp arrays
in PyTorch autograd computations.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_ctype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L186-L299"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_torch" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Torch tensor to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)"><em>torch.Tensor</em></a>) – The torch tensor to wrap.</p></li>
<li><p><strong>dtype</strong> (<em>warp.dtype</em><em>, </em><em>optional</em>) – The target data type of the resulting Warp array. Defaults to the tensor value type mapped to a Warp array value type.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting array should wrap the tensor’s gradient, if it exists (the grad tensor will be allocated otherwise). Defaults to the tensor’s <cite>requires_grad</cite> value.</p></li>
<li><p><strong>return_ctype</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to return a low-level array descriptor instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object (faster).  The descriptor can be passed to Warp kernels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The wrapped array or array descriptor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L301-L344"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_torch" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Warp array to a Torch tensor without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting tensor should convert the array’s gradient, if it exists, to a grad tensor. Defaults to the array’s <cite>requires_grad</cite> value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.4)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L17-L48"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_from_torch" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp device corresponding to a Torch device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>torch_device</strong> (<cite>torch.device</cite> or <cite>str</cite>) – Torch device identifier</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Torch device does not have a corresponding Warp device</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L50-L66"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_to_torch" title="Link to this definition">¶</a></dt>
<dd><p>Return the Torch device string corresponding to a Warp device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – An identifier that can be resolved to a <a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.context.Device</span></code></a>.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The Warp device is not compatible with PyTorch.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L105-L142"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_torch" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp dtype corresponding to a Torch dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>torch_dtype</strong> – A <code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> that has a corresponding Warp data type.
Currently <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.complex64</span></code>, and
<code class="docutils literal notranslate"><span class="pre">torch.complex128</span></code> are not supported.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Warp data type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L68-L103"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_torch" title="Link to this definition">¶</a></dt>
<dd><p>Return the Torch dtype corresponding to a Warp dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_dtype</strong> – A Warp data type that has a corresponding <code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code>.
<code class="docutils literal notranslate"><span class="pre">warp.uint16</span></code>, <code class="docutils literal notranslate"><span class="pre">warp.uint32</span></code>, and <code class="docutils literal notranslate"><span class="pre">warp.uint64</span></code> are mapped
to the signed integer <code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> of the same width.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding PyTorch data type.</p>
</dd>
</dl>
</dd></dl>

<p>To convert a PyTorch CUDA stream to a Warp CUDA stream and vice versa, Warp provides the following functions:</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.stream_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">stream_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream_or_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L346-L364"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.stream_from_torch" title="Link to this definition">¶</a></dt>
<dd><p>Convert from a Torch CUDA stream to a Warp CUDA stream.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.stream_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">stream_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream_or_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/torch.py#L366-L384"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.stream_to_torch" title="Link to this definition">¶</a></dt>
<dd><p>Convert from a Warp CUDA stream to a Torch CUDA stream.</p>
</dd></dl>

<section id="example-optimization-using-warp-from-torch">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_torch()</span></code><a class="headerlink" href="#example-optimization-using-warp-from-torch" title="Link to this heading">¶</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via PyTorch’s Adam optimizer
using <a class="reference internal" href="#warp.from_torch" title="warp.from_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xs</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-torch">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code><a class="headerlink" href="#example-optimization-using-warp-to-torch" title="Link to this heading">¶</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <a class="reference internal" href="#warp.to_torch" title="warp.to_torch"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_torch()</span></code></a> to convert them to PyTorch tensors.
Here, we revisit the same example from above where now only a single conversion to a torch tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_torch call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-torch-autograd-function">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code><a class="headerlink" href="#example-optimization-using-torch-autograd-function" title="Link to this heading">¶</a></h3>
<p>One can insert Warp kernel launches in a PyTorch graph by defining a <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code></a> class, which
requires forward and backward functions to be defined. After mapping incoming torch arrays to Warp arrays, a Warp kernel
may be launched in the usual way. In the backward pass, the same kernel’s adjoint may be launched by
setting <code class="docutils literal notranslate"><span class="pre">adjoint</span> <span class="pre">=</span> <span class="pre">True</span></code> in <code class="xref py py-func docutils literal notranslate"><span class="pre">wp.launch()</span></code>. Alternatively, the user may choose to rely on Warp’s tape.
In the following example, we demonstrate how Warp may be used to evaluate the Rosenbrock function in an optimization context:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">pvec2</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define the Rosenbrock function</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">func</span>
<span class="k">def</span> <span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">eval_rosenbrock</span><span class="p">(</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">pvec2</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Rosenbrock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pvec2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">num_points</span>

        <span class="c1"># allocate output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
        <span class="c1"># map incoming Torch grads to our output variables</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">adj_z</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">],</span>
            <span class="n">adj_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adj_outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adjoint</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># return adjoint w.r.t. inputs</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>


<span class="n">num_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-2</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_to_torch</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xy</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Rosenbrock</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># minimum at (1, 1)</span>
<span class="n">xy_np</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xy_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that if Warp code is wrapped in a torch.autograd.function that gets called in <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, it will automatically
exclude that function from compiler optimizations. If your script uses <code class="docutils literal notranslate"><span class="pre">torch.compile()</span></code>, we recommend using Pytorch version 2.3.0+,
which has improvements that address this scenario.</p>
</section>
<section id="performance-notes">
<h3>Performance Notes<a class="headerlink" href="#performance-notes" title="Link to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">wp.from_torch()</span></code> function creates a Warp array object that shares data with a PyTorch tensor.  Although this function does not copy the data, there is always some CPU overhead during the conversion.  If these conversions happen frequently, the overall program performance may suffer.  As a general rule, it’s good to avoid repeated conversions of the same tensor.  Instead of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
    <span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Try converting the arrays only once and reuse them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>If reusing arrays is not possible (e.g., a new PyTorch tensor is constructed on every iteration), passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> to <code class="docutils literal notranslate"><span class="pre">wp.from_torch()</span></code> should yield faster performance.  Setting this argument to True avoids constructing a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object and instead returns a low-level array descriptor.  This descriptor is a simple C structure that can be passed to Warp kernels instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>, but cannot be used in other places that require a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="c1"># get Torch tensors for this iteration</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># get Warp array descriptors</span>
    <span class="n">x_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_ctype</span><span class="p">,</span> <span class="n">y_ctype</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative approach is to pass the PyTorch tensors to Warp kernels directly.  This avoids constructing temporary Warp arrays by leveraging standard array interfaces (like <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>) supported by both PyTorch and Warp.  The main advantage of this approach is convenience, since there is no need to call any conversion functions.  The main limitation is that it does not handle gradients, because gradient information is not included in the standard array interfaces.  This technique is therefore most suitable for algorithms that do not involve differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>warp.examples.benchmarks.benchmark_interop_torch
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">5095</span> <span class="n">ms</span>  <span class="n">from_torch</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="mi">2113</span> <span class="n">ms</span>  <span class="n">from_torch</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">2950</span> <span class="n">ms</span>  <span class="n">direct</span> <span class="kn">from</span> <span class="nn">torch</span>
</pre></div>
</div>
<p>The default <code class="docutils literal notranslate"><span class="pre">wp.from_torch()</span></code> conversion is the slowest.  Passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> is the fastest, because it skips creating temporary Warp array objects.  Passing PyTorch tensors to Warp kernels directly falls somewhere in between.  It skips creating temporary Warp arrays, but accessing the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> attributes of PyTorch tensors adds overhead because they are initialized on-demand.</p>
</section>
</section>
<section id="cupy-numba">
<h2>CuPy/Numba<a class="headerlink" href="#cupy-numba" title="Link to this heading">¶</a></h2>
<p>Warp GPU arrays support the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocol for sharing data with other Python GPU frameworks.  This allows frameworks like CuPy to use Warp GPU arrays directly.</p>
<p>Likewise, Warp arrays can be created from any object that exposes the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>.  Such objects can also be passed to Warp kernels directly without creating a Warp array object.</p>
</section>
<section id="jax">
<span id="jax-interop"></span><h2>JAX<a class="headerlink" href="#jax" title="Link to this heading">¶</a></h2>
<p>Interoperability with JAX arrays is supported through the following methods.
Internally these use the DLPack protocol to exchange data in a zero-copy way with JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_jax</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>
<span class="n">jax_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_jax</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>It may be preferable to use the <a class="reference internal" href="#dlpack"><span class="std std-ref">DLPack</span></a> protocol directly for better performance and control over stream synchronization behaviour.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_array</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/jax.py#L154-L167"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_jax" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Jax array to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>jax_array</strong> (<a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array" title="(in JAX)"><em>jax.Array</em></a>) – The Jax array to convert.</p></li>
<li><p><strong>dtype</strong> (<em>optional</em>) – The target data type of the resulting Warp array. Defaults to the Jax array’s data type mapped to a Warp data type.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted Warp array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_array</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/jax.py#L139-L152"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_jax" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Warp array to a Jax array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_array</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted Jax array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array" title="(in JAX)">jax.Array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/jax.py#L36-L51"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_from_jax" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp device corresponding to a Jax device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>jax_device</strong> (<a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html#jax.Device" title="(in JAX)"><em>jax.Device</em></a>) – A Jax device descriptor.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The Jax device is neither a CPU nor GPU device.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/jax.py#L11-L34"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_to_jax" title="Link to this definition">¶</a></dt>
<dd><p>Return the Jax device corresponding to a Warp device.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html#jax.Device" title="(in JAX)"><code class="xref py py-class docutils literal notranslate"><span class="pre">jax.Device</span></code></a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Failed to find the corresponding Jax device.</p>
</dd>
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/jax.py#L88-L132"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_jax" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp dtype corresponding to a Jax dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Warp data type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/jax.py#L53-L86"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_jax" title="Link to this definition">¶</a></dt>
<dd><p>Return the Jax dtype corresponding to a Warp dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_dtype</strong> – A Warp data type that has a corresponding Jax data type.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Jax data type.</p>
</dd>
</dl>
</dd></dl>

<section id="using-warp-kernels-as-jax-primitives">
<h3>Using Warp kernels as JAX primitives<a class="headerlink" href="#using-warp-kernels-as-jax-primitives" title="Link to this heading">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an experimental feature under development.</p>
</div>
<p>Warp kernels can be used as JAX primitives, which can be used to call Warp kernels inside of jitted JAX functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jp</span>

<span class="c1"># import experimental feature</span>
<span class="kn">from</span> <span class="nn">warp.jax_experimental</span> <span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">triple_kernel</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_triple</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">triple_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_triple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<p>Since this is an experimental feature, there are some limitations:</p>
<blockquote>
<div><ul class="simple">
<li><p>All kernel arguments must be arrays.</p></li>
<li><p>Kernel launch dimensions are inferred from the shape of the first argument.</p></li>
<li><p>Input arguments are followed by output arguments in the Warp kernel definition.</p></li>
<li><p>There must be at least one input argument and at least one output argument.</p></li>
<li><p>All arrays must be contiguous.</p></li>
<li><p>Only the CUDA backend is supported.</p></li>
</ul>
</div></blockquote>
<p>Here is an example of an operation with three inputs and two outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jp</span>

<span class="c1"># import experimental feature</span>
<span class="kn">from</span> <span class="nn">warp.jax_experimental</span> <span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="c1"># kernel with multiple inputs and outputs</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">multiarg_kernel</span><span class="p">(</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">ab</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">bc</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">ab</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>
    <span class="n">bc</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_multiarg</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multiarg_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function with three inputs and two outputs</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_multiarg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-shardmap-for-distributed-computation">
<h3>Using shardmap for distributed computation<a class="headerlink" href="#using-shardmap-for-distributed-computation" title="Link to this heading">¶</a></h3>
<p>Warp can be used in conjunction with JAX’s <a class="reference external" href="https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html">shard_map</a> to perform distributed multi-GPU computations.</p>
<p>To achieve this, the JAX distributed environment must be initialized (see <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed Arrays and Automatic Parallelization</a> for more details):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
</pre></div>
</div>
<p>This initialization must be called at the beginning of your program, before any other JAX operations.</p>
<p>Here’s an example of how to use <cite>shard_map</cite> with a Warp kernel:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax.sharding</span> <span class="kn">import</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">jax.experimental.multihost_utils</span> <span class="kn">import</span> <span class="n">process_allgather</span> <span class="k">as</span> <span class="n">allgather</span>
<span class="kn">from</span> <span class="nn">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">shard_map</span>
<span class="kn">from</span> <span class="nn">warp.jax_experimental</span> <span class="kn">import</span> <span class="n">jax_kernel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Initialize JAX distributed environment</span>
<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">print_on_process_0</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2"> GPU(s)&quot;</span><span class="p">)</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">multiply_by_two_kernel</span><span class="p">(</span>
    <span class="n">a_in</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">a_out</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">a_out</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_in</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">*</span> <span class="mf">2.0</span>

<span class="n">jax_warp_multiply</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multiply_by_two_kernel</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">warp_multiply</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">jax_warp_multiply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># a_in here is the full sharded array with shape (M,)</span>
    <span class="c1"># The output will also be a sharded array with shape (M,)</span>
<span class="k">def</span> <span class="nf">warp_distributed_operator</span><span class="p">(</span><span class="n">a_in</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_sharded_operator</span><span class="p">(</span><span class="n">a_in</span><span class="p">):</span>
        <span class="c1"># Inside the sharded operator, a_in is a local shard on each device</span>
        <span class="c1"># If we have N devices and input size M, each shard has shape (M/N,)</span>

        <span class="c1"># warp_multiply applies the Warp kernel to the local shard</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">warp_multiply</span><span class="p">(</span><span class="n">a_in</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># result has the same shape as the input shard (M/N,)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># shard_map distributes the computation across devices</span>
    <span class="k">return</span> <span class="n">shard_map</span><span class="p">(</span>
        <span class="n">_sharded_operator</span><span class="p">,</span>
        <span class="n">mesh</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()),</span> <span class="s2">&quot;x&quot;</span><span class="p">),</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),),</span>  <span class="c1"># Input is sharded along the &#39;x&#39; axis</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span>    <span class="c1"># Output is also sharded along the &#39;x&#39; axis</span>
        <span class="n">check_rep</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">a_in</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Test distributed multiplication using JAX + Warp&quot;</span><span class="p">)</span>

<span class="n">devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">sharding_spec</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>

<span class="n">input_size</span> <span class="o">=</span> <span class="n">num_gpus</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># 5 elements per device</span>
<span class="n">single_device_arrays</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define the shape of the input array based on the total input size</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,)</span>

<span class="c1"># Create a list of arrays by distributing the single_device_arrays across the available devices</span>
<span class="c1"># Each device will receive a portion of the input data</span>
<span class="n">arrays</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">single_device_arrays</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">d</span><span class="p">)</span>  <span class="c1"># Place each element on the corresponding device</span>
    <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sharding_spec</span><span class="o">.</span><span class="n">addressable_devices_indices_map</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="p">]</span>

<span class="c1"># Combine the individual device arrays into a single sharded array</span>
<span class="n">sharded_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">make_array_from_single_device_arrays</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">sharding_spec</span><span class="p">,</span> <span class="n">arrays</span><span class="p">)</span>

<span class="c1"># sharded_array has shape (input_size,) but is distributed across devices</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input array: </span><span class="si">{</span><span class="n">allgather</span><span class="p">(</span><span class="n">sharded_array</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># warp_result has the same shape and sharding as sharded_array</span>
<span class="n">warp_result</span> <span class="o">=</span> <span class="n">warp_distributed_operator</span><span class="p">(</span><span class="n">sharded_array</span><span class="p">)</span>

<span class="c1"># allgather collects results from all devices, resulting in a full array of shape (input_size,)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Warp Output:&quot;</span><span class="p">,</span> <span class="n">allgather</span><span class="p">(</span><span class="n">warp_result</span><span class="p">))</span>
</pre></div>
</div>
<p>In this example, <cite>shard_map</cite> is used to distribute the computation across available devices. The input array <cite>a_in</cite> is sharded along the ‘x’ axis, and each device processes its local shard. The Warp kernel <cite>multiply_by_two_kernel</cite> is applied to each shard, and the results are combined to form the final output.</p>
<p>This approach allows for efficient parallel processing of large arrays, as each device works on a portion of the data simultaneously.</p>
<p>To run this program on multiple GPUs, you must have OpenMPI installed. You can consult the <a class="reference external" href="https://docs.open-mpi.org/en/v5.0.x/installing-open-mpi/quickstart.html">OpenMPI installation guide</a> for instructions on how to install it. Once OpenMPI is installed, you can use <cite>mpirun</cite> with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-np<span class="w"> </span>&lt;NUM_OF_GPUS&gt;<span class="w"> </span>python<span class="w"> </span>&lt;filename&gt;.py
</pre></div>
</div>
</section>
<section id="specifying-launch-dimensions-for-matrix-operations">
<h3>Specifying launch dimensions for matrix operations<a class="headerlink" href="#specifying-launch-dimensions-for-matrix-operations" title="Link to this heading">¶</a></h3>
<p>In some cases, particularly for matrix operations, it’s necessary to specify the launch dimensions for Warp kernels. This is because the default behavior of inferring dimensions from the first argument may not always be suitable for matrix operations. Here’s an example of a distributed matrix multiplication using Warp and JAX:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax.sharding</span> <span class="kn">import</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">jax.experimental.multihost_utils</span> <span class="kn">import</span> <span class="n">process_allgather</span> <span class="k">as</span> <span class="n">allgather</span>
<span class="kn">from</span> <span class="nn">jax.experimental.shard_map</span> <span class="kn">import</span> <span class="n">shard_map</span>
<span class="kn">from</span> <span class="nn">warp.jax_experimental</span> <span class="kn">import</span> <span class="n">jax_kernel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">jax</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">print_on_process_0</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on </span><span class="si">{</span><span class="n">num_gpus</span><span class="si">}</span><span class="s2"> GPU(s)&quot;</span><span class="p">)</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">matmul_kernel</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array2d</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
<span class="p">):</span>
    <span class="c1"># a: (M/num_gpus, K), b: (K, N), c: (M/num_gpus, N)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># M/num_gpus</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># K</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># N</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span> <span class="ow">and</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
        <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

<span class="c1"># Specify launch dimensions based on the number of GPUs</span>
<span class="k">def</span> <span class="nf">create_jax_warp_matmul</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="c1"># M: total rows, N: total columns</span>
    <span class="n">block_size_m</span> <span class="o">=</span> <span class="n">M</span> <span class="o">//</span> <span class="n">num_gpus</span>  <span class="c1"># Rows per GPU</span>
    <span class="n">block_size_n</span> <span class="o">=</span> <span class="n">N</span>  <span class="c1"># All columns</span>
    <span class="k">return</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">matmul_kernel</span><span class="p">,</span> <span class="n">launch_dims</span><span class="o">=</span><span class="p">(</span><span class="n">block_size_m</span><span class="p">,</span> <span class="n">block_size_n</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">warp_distributed_matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># a: (M, K) sharded across GPUs, b: (K, N) replicated</span>
    <span class="n">M</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">jax_warp_matmul</span> <span class="o">=</span> <span class="n">create_jax_warp_matmul</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sharded_operator</span><span class="p">(</span><span class="n">a_shard</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="c1"># a_shard: (M/num_gpus, K), b: (K, N)</span>
        <span class="k">return</span> <span class="n">jax_warp_matmul</span><span class="p">(</span><span class="n">a_shard</span><span class="p">,</span> <span class="n">b</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Result: (M/num_gpus, N)</span>

    <span class="k">return</span> <span class="n">shard_map</span><span class="p">(</span>
        <span class="n">_sharded_operator</span><span class="p">,</span>
        <span class="n">mesh</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()),</span> <span class="s2">&quot;x&quot;</span><span class="p">),</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)),</span>  <span class="c1"># a sharded in first dim, b replicated</span>
        <span class="n">out_specs</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>  <span class="c1"># Output sharded in first dim</span>
        <span class="n">check_rep</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Test distributed matrix multiplication using JAX + Warp&quot;</span><span class="p">)</span>

<span class="c1"># Define matrix dimensions</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">num_gpus</span>  <span class="c1"># Scale M with the number of devices</span>
<span class="n">K</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span>

<span class="c1"># Create input matrices</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>  <span class="c1"># Shape: (M, K)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">K</span> <span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>  <span class="c1"># Shape: (K, N)</span>

<span class="n">devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">sharding_spec_a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="n">sharding_spec_b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

<span class="c1"># Shard matrix A and replicate matrix B</span>
<span class="n">sharded_a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">sharding_spec_a</span><span class="p">)</span>  <span class="c1"># Sharded shape: (M/num_gpus, K) per device</span>
<span class="n">replicated_b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">sharding_spec_b</span><span class="p">)</span>  <span class="c1"># Replicated shape: (K, N) on all devices</span>

<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input matrix A:</span><span class="se">\n</span><span class="si">{</span><span class="n">allgather</span><span class="p">(</span><span class="n">sharded_a</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Shape: (M, K)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input matrix B:</span><span class="se">\n</span><span class="si">{</span><span class="n">allgather</span><span class="p">(</span><span class="n">replicated_b</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Shape: (K, N)</span>

<span class="n">warp_result</span> <span class="o">=</span> <span class="n">warp_distributed_matmul</span><span class="p">(</span><span class="n">sharded_a</span><span class="p">,</span> <span class="n">replicated_b</span><span class="p">)</span>  <span class="c1"># Sharded result: (M/num_gpus, N) per device</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;Warp Output:&quot;</span><span class="p">)</span>
<span class="c1"># Use allgather to collect results from all devices</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="n">allgather</span><span class="p">(</span><span class="n">warp_result</span><span class="p">))</span>  <span class="c1"># Shape: (M, N)</span>

<span class="n">jax_result</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># Shape: (M, N)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="s2">&quot;JAX Output:&quot;</span><span class="p">)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="n">jax_result</span><span class="p">)</span>

<span class="n">expected_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected shape: </span><span class="si">{</span><span class="n">expected_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warp output shape: </span><span class="si">{</span><span class="n">warp_result</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be (M/num_gpus, N) on each device</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;JAX output shape: </span><span class="si">{</span><span class="n">jax_result</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be (M, N)</span>

<span class="n">allclose</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">allgather</span><span class="p">(</span><span class="n">warp_result</span><span class="p">),</span> <span class="n">jax_result</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">print_on_process_0</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Allclose: </span><span class="si">{</span><span class="n">allclose</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, we create a function <cite>create_jax_warp_matmul</cite> that calculates the launch dimensions based on the number of available GPUs. We use <cite>jax.device_count()</cite> to get the global number of GPUs and divide the <cite>M</cite> dimension (rows) of the matrix by this number. This ensures that each GPU processes an equal portion of the input matrix A. The <cite>N</cite> dimension (columns) remains unchanged as we’re not sharding in that direction.</p>
<p>Note that the launch dimensions are set to match the shape of the matrix portion on each GPU. The <cite>block_size_m</cite> is calculated by dividing the total number of rows by the number of GPUs, while <cite>block_size_n</cite> is set to the full width of the output matrix.</p>
<p>Note that this is a naive implementation of matrix multiplication for the sake of this illustration, and there are many optimizations that can be made to improve performance.</p>
</section>
</section>
<section id="dlpack">
<span id="id1"></span><h2>DLPack<a class="headerlink" href="#dlpack" title="Link to this heading">¶</a></h2>
<p>Warp supports the DLPack protocol included in the Python Array API standard v2022.12.
See the <a class="reference external" href="https://dmlc.github.io/dlpack/latest/python_spec.html">Python Specification for DLPack</a> for reference.</p>
<p>The canonical way to import an external array into Warp is using the <code class="docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">external_array</span><span class="p">)</span>
</pre></div>
</div>
<p>The external array can be a PyTorch tensor, Jax array, or any other array type compatible with this version of the DLPack protocol.
For CUDA arrays, this approach requires the producer to perform stream synchronization which ensures that operations on the array
are ordered correctly.  The <code class="docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code> function asks the producer to synchronize the current Warp stream on the device where
the array resides.  Thus it should be safe to use the array in Warp kernels on that device without any additional synchronization.</p>
<p>The canonical way to export a Warp array to an external framework is to use the <code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code> function in that framework:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">paddle_tensor</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>For CUDA arrays, this will synchronize the current stream of the consumer framework with the current Warp stream on the array’s device.
Thus it should be safe to use the wrapped array in the consumer framework, even if the array was previously used in a Warp kernel
on the device.</p>
<p>Alternatively, arrays can be shared by explicitly creating PyCapsules using a <code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code> function provided by the producer framework.
This approach may be used for older versions of frameworks that do not support the v2022.12 standard:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array1</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">jax_array</span><span class="p">))</span>
<span class="n">warp_array2</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">))</span>
<span class="n">warp_array3</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">paddle_tensor</span><span class="p">))</span>

<span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">paddle_tensor</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
</pre></div>
</div>
<p>This approach is generally faster because it skips any stream synchronization, but another solution must be used to ensure correct
ordering of operations.  In situations where no synchronization is required, using this approach can yield better performance.
This may be a good choice in situations like these:</p>
<blockquote>
<div><ul class="simple">
<li><p>The external framework is using the synchronous CUDA default stream.</p></li>
<li><p>Warp and the external framework are using the same CUDA stream.</p></li>
<li><p>Another synchronization mechanism is already in place.</p></li>
</ul>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_dlpack">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_dlpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/dlpack.py#L415-L453"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_dlpack" title="Link to this definition">¶</a></dt>
<dd><p>Convert a source array or DLPack capsule into a Warp array without copying.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> – A DLPack-compatible array or PyCapsule</p></li>
<li><p><strong>dtype</strong> – An optional Warp data type to interpret the source data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Warp array that uses the same underlying memory as the input
pycapsule.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_dlpack">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_dlpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wp_array</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/dlpack.py#L220-L304"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_dlpack" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Warp array to another type of DLPack-compatible array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>wp_array</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a>) – The source Warp array that will be converted.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A capsule containing a DLManagedTensor that can be converted
to another array type without copying the underlying memory.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="paddle">
<span id="paddle-interop"></span><h2>Paddle<a class="headerlink" href="#paddle" title="Link to this heading">¶</a></h2>
<p>Warp provides helper functions to convert arrays to/from Paddle:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Paddle tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_paddle</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Paddle tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from Paddle tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from Paddle autograd tensors, allowing the use of Warp arrays
in Paddle autograd computations.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_ctype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L194-L315"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.from_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Paddle tensor to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<em>paddle.Tensor</em>) – The paddle tensor to wrap.</p></li>
<li><p><strong>dtype</strong> (<em>warp.dtype</em><em>, </em><em>optional</em>) – The target data type of the resulting Warp array. Defaults to the tensor value type mapped to a Warp array value type.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting array should wrap the tensor’s gradient, if it exists (the grad tensor will be allocated otherwise). Defaults to the tensor’s <cite>requires_grad</cite> value.</p></li>
<li><p><strong>grad</strong> (<em>paddle.Tensor</em><em>, </em><em>optional</em>) – The grad attached to given tensor. Defaults to None.</p></li>
<li><p><strong>return_ctype</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to return a low-level array descriptor instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object (faster).  The descriptor can be passed to Warp kernels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The wrapped array or array descriptor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L317-L363"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.to_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Convert a Warp array to a Paddle tensor without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting tensor should convert the array’s gradient, if it exists, to a grad tensor. Defaults to the array’s <cite>requires_grad</cite> value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>paddle.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paddle_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L23-L56"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_from_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp device corresponding to a Paddle device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>paddle_device</strong> (<cite>paddle.base.libpaddle.Place</cite> or <cite>str</cite>) – Paddle device identifier</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – Paddle device does not have a corresponding Warp device</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device">warp.context.Device</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L58-L74"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.device_to_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Return the Paddle device string corresponding to a Warp device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_device</strong> (<a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em> | </em><em>None</em>) – An identifier that can be resolved to a <a class="reference internal" href="devices.html#warp.context.Device" title="warp.context.Device"><code class="xref py py-class docutils literal notranslate"><span class="pre">warp.context.Device</span></code></a>.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.13)"><strong>RuntimeError</strong></a> – The Warp device is not compatible with PyPaddle.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paddle_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L113-L150"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_from_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Return the Warp dtype corresponding to a Paddle dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>paddle_dtype</strong> – A <code class="docutils literal notranslate"><span class="pre">paddle.dtype</span></code> that has a corresponding Warp data type.
Currently <code class="docutils literal notranslate"><span class="pre">paddle.bfloat16</span></code>, <code class="docutils literal notranslate"><span class="pre">paddle.complex64</span></code>, and
<code class="docutils literal notranslate"><span class="pre">paddle.complex128</span></code> are not supported.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding Warp data type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L76-L111"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.dtype_to_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Return the Paddle dtype corresponding to a Warp dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_dtype</strong> – A Warp data type that has a corresponding <code class="docutils literal notranslate"><span class="pre">paddle.dtype</span></code>.
<code class="docutils literal notranslate"><span class="pre">warp.uint16</span></code>, <code class="docutils literal notranslate"><span class="pre">warp.uint32</span></code>, and <code class="docutils literal notranslate"><span class="pre">warp.uint64</span></code> are mapped
to the signed integer <code class="docutils literal notranslate"><span class="pre">paddle.dtype</span></code> of the same width.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.13)"><strong>TypeError</strong></a> – Unable to find a corresponding PyPaddle data type.</p>
</dd>
</dl>
</dd></dl>

<p>To convert a Paddle CUDA stream to a Warp CUDA stream and vice versa, Warp provides the following functions:</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.stream_from_paddle">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">stream_from_paddle</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stream_or_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/NVIDIA/warp/blob/v1.4.1/warp/paddle.py#L365-L383"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#warp.stream_from_paddle" title="Link to this definition">¶</a></dt>
<dd><p>Convert from a Paddle CUDA stream to a Warp CUDA stream.</p>
</dd></dl>

<section id="example-optimization-using-warp-from-paddle">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code><a class="headerlink" href="#example-optimization-using-warp-from-paddle" title="Link to this heading">¶</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via Paddle’s Adam optimizer
using <a class="reference internal" href="#warp.from_paddle" title="warp.from_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">paddle</span>

<span class="c1"># init warp context at beginning</span>
<span class="n">wp</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">xs</span><span class="o">.</span><span class="n">stop_gradient</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="n">l</span><span class="o">.</span><span class="n">stop_gradient</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">])</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-paddle">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_paddle</span></code><a class="headerlink" href="#example-optimization-using-warp-to-paddle" title="Link to this heading">¶</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <a class="reference internal" href="#warp.to_paddle" title="warp.to_paddle"><code class="xref py py-func docutils literal notranslate"><span class="pre">warp.to_paddle()</span></code></a> to convert them to Paddle tensors.
Here, we revisit the same example from above where now only a single conversion to a paddle tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">paddle</span>

<span class="c1"># init warp context at beginning</span>
<span class="n">wp</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_paddle call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="p">[</span><span class="n">wp</span><span class="o">.</span><span class="n">to_paddle</span><span class="p">(</span><span class="n">xs</span><span class="p">)])</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Performance Notes<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code> function creates a Warp array object that shares data with a Paddle tensor.  Although this function does not copy the data, there is always some CPU overhead during the conversion.  If these conversions happen frequently, the overall program performance may suffer.  As a general rule, it’s good to avoid repeated conversions of the same tensor.  Instead of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
    <span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>Try converting the arrays only once and reuse them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="n">x_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="n">y_w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_w</span><span class="p">,</span> <span class="n">y_w</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>If reusing arrays is not possible (e.g., a new Paddle tensor is constructed on every iteration), passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> to <code class="docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code> should yield faster performance.  Setting this argument to True avoids constructing a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code> object and instead returns a low-level array descriptor.  This descriptor is a simple C structure that can be passed to Warp kernels instead of a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>, but cannot be used in other places that require a <code class="docutils literal notranslate"><span class="pre">wp.array</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="c1"># get Paddle tensors for this iteration</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">y_t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># get Warp array descriptors</span>
    <span class="n">x_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_ctype</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_paddle</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x_ctype</span><span class="p">,</span> <span class="n">y_ctype</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>An alternative approach is to pass the Paddle tensors to Warp kernels directly.  This avoids constructing temporary Warp arrays by leveraging standard array interfaces (like <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code>) supported by both Paddle and Warp.  The main advantage of this approach is convenience, since there is no need to call any conversion functions.  The main limitation is that it does not handle gradients, because gradient information is not included in the standard array interfaces.  This technique is therefore most suitable for algorithms that do not involve differentiation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">paddle</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">device_to_paddle</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">saxpy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>warp.examples.benchmarks.benchmark_interop_paddle
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">13990</span> <span class="n">ms</span>  <span class="n">from_paddle</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
 <span class="mi">5990</span> <span class="n">ms</span>  <span class="n">from_paddle</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">return_ctype</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="mi">35167</span> <span class="n">ms</span>  <span class="n">direct</span> <span class="kn">from</span> <span class="nn">paddle</span>
</pre></div>
</div>
<p>The default <code class="docutils literal notranslate"><span class="pre">wp.from_paddle()</span></code> conversion is the slowest.  Passing <code class="docutils literal notranslate"><span class="pre">return_ctype=True</span></code> is the fastest, because it skips creating temporary Warp array objects.  Passing Paddle tensors to Warp kernels directly falls somewhere in between.  It skips creating temporary Warp arrays, but accessing the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> attributes of Paddle tensors adds overhead because they are initialized on-demand.</p>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../configuration.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Configuration</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="generics.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Generics</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022-2024, NVIDIA Corporation
            </div>
            Made with 
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/NVIDIA/warp" aria-label="GitHub">
            <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
            </svg>
        </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Interoperability</a><ul>
<li><a class="reference internal" href="#numpy">NumPy</a><ul>
<li><a class="reference internal" href="#warp.from_numpy"><code class="docutils literal notranslate"><span class="pre">from_numpy()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_from_numpy"><code class="docutils literal notranslate"><span class="pre">dtype_from_numpy()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_numpy"><code class="docutils literal notranslate"><span class="pre">dtype_to_numpy()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch">PyTorch</a><ul>
<li><a class="reference internal" href="#warp.from_torch"><code class="docutils literal notranslate"><span class="pre">from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_torch"><code class="docutils literal notranslate"><span class="pre">to_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_from_torch"><code class="docutils literal notranslate"><span class="pre">device_from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_to_torch"><code class="docutils literal notranslate"><span class="pre">device_to_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_from_torch"><code class="docutils literal notranslate"><span class="pre">dtype_from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_torch"><code class="docutils literal notranslate"><span class="pre">dtype_to_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.stream_from_torch"><code class="docutils literal notranslate"><span class="pre">stream_from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.stream_to_torch"><code class="docutils literal notranslate"><span class="pre">stream_to_torch()</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-warp-from-torch">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-warp-to-torch">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-torch-autograd-function">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code></a></li>
<li><a class="reference internal" href="#performance-notes">Performance Notes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cupy-numba">CuPy/Numba</a></li>
<li><a class="reference internal" href="#jax">JAX</a><ul>
<li><a class="reference internal" href="#warp.from_jax"><code class="docutils literal notranslate"><span class="pre">from_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_jax"><code class="docutils literal notranslate"><span class="pre">to_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_from_jax"><code class="docutils literal notranslate"><span class="pre">device_from_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_to_jax"><code class="docutils literal notranslate"><span class="pre">device_to_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_from_jax"><code class="docutils literal notranslate"><span class="pre">dtype_from_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_jax"><code class="docutils literal notranslate"><span class="pre">dtype_to_jax()</span></code></a></li>
<li><a class="reference internal" href="#using-warp-kernels-as-jax-primitives">Using Warp kernels as JAX primitives</a></li>
<li><a class="reference internal" href="#using-shardmap-for-distributed-computation">Using shardmap for distributed computation</a></li>
<li><a class="reference internal" href="#specifying-launch-dimensions-for-matrix-operations">Specifying launch dimensions for matrix operations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dlpack">DLPack</a><ul>
<li><a class="reference internal" href="#warp.from_dlpack"><code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_dlpack"><code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#paddle">Paddle</a><ul>
<li><a class="reference internal" href="#warp.from_paddle"><code class="docutils literal notranslate"><span class="pre">from_paddle()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_paddle"><code class="docutils literal notranslate"><span class="pre">to_paddle()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_from_paddle"><code class="docutils literal notranslate"><span class="pre">device_from_paddle()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_to_paddle"><code class="docutils literal notranslate"><span class="pre">device_to_paddle()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_from_paddle"><code class="docutils literal notranslate"><span class="pre">dtype_from_paddle()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_paddle"><code class="docutils literal notranslate"><span class="pre">dtype_to_paddle()</span></code></a></li>
<li><a class="reference internal" href="#warp.stream_from_paddle"><code class="docutils literal notranslate"><span class="pre">stream_from_paddle()</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-warp-from-paddle">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_paddle()</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-warp-to-paddle">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_paddle</span></code></a></li>
<li><a class="reference internal" href="#id2">Performance Notes</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=350a9c04"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    </body>
</html>